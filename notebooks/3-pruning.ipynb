{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gaussian_pixels', 'random_labels', 'random_pixels', 'partial_labels', 'shuffled_pixels']\n",
      "['resnet18', 'resnet34', 'alexnet', 'inception', 'mlp_1x512', 'mlp_3x512']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from model import LitModel\n",
    "from main import (\n",
    "    build_experiment,\n",
    "    DEFAULT_PARAMS,\n",
    "    get_cifar_models,\n",
    "    available_corruptions,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "all_corruptions = available_corruptions()\n",
    "print(all_corruptions)\n",
    "\n",
    "models = get_cifar_models(lib=\"torch\")\n",
    "\n",
    "print(list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.core.module import LightningModule\n",
    "from lightning.pytorch.callbacks import ModelPruning\n",
    "from lightning.pytorch.utilities.rank_zero import rank_zero_debug\n",
    "\n",
    "class LitModelPruning(ModelPruning):\n",
    "    def filter_parameters_to_prune(self, parameters_to_prune):\n",
    "        # filter linear layers\n",
    "        filter_names = [\"Linear\"]\n",
    "        return [\n",
    "            (param, name)\n",
    "            for param, name in parameters_to_prune\n",
    "            if not any([filter_name in param.__class__.__name__ for filter_name in filter_names])\n",
    "        ]\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: LightningModule) -> None:\n",
    "        if self._prune_on_train_epoch_end:\n",
    "            rank_zero_debug(\"`ModelPruning.on_train_epoch_end`. Applying pruning\")\n",
    "            self._run_pruning(pl_module.current_epoch)\n",
    "\n",
    "def apply_prunning_every(k):\n",
    "    \"\"\"\n",
    "    Apply pruning every k epochs, ignores epoch 0.\n",
    "    \n",
    "    Parameters:\n",
    "        k (int): apply pruning every k epochs\n",
    "\n",
    "    Returns:\n",
    "        Callable: wrapper with the following signature: `wrapper(current_epoch) -> bool`\n",
    "    \"\"\"\n",
    "\n",
    "    return lambda current_epoch: current_epoch % k == 0 and current_epoch != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">alexnet_normal_labels_0.0</strong> at: <a href='https://wandb.ai/stepp1/pruning/runs/7fy7369x' target=\"_blank\">https://wandb.ai/stepp1/pruning/runs/7fy7369x</a><br/>Synced 7 W&B file(s), 0 media file(s), 5 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230628_144806-7fy7369x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "hparams = DEFAULT_PARAMS.copy()\n",
    "\n",
    "MODEL_NAME = \"alexnet\"\n",
    "CORRUPTION_NAME = \"normal_labels\"\n",
    "CORRUPTION_PROB = 0.0\n",
    "\n",
    "model = models[MODEL_NAME]\n",
    "\n",
    "hparams[\"model_name\"] = MODEL_NAME\n",
    "hparams[\"n_classes\"] = 10\n",
    "hparams[\"drop_return_index\"] = True\n",
    "hparams[\"corrupt_name\"] = CORRUPTION_NAME\n",
    "hparams[\"corrupt_prob\"] = CORRUPTION_PROB\n",
    "hparams[\"val_every\"] = 1\n",
    "\n",
    "\n",
    "experiment_name = f\"{hparams['model_name']}_{hparams['corrupt_name']}_{hparams['corrupt_prob']}\"\n",
    "\n",
    "os.environ.update({\"WANDB_NOTEBOOK_NAME\": \"pruning.ipynb\"})\n",
    "\n",
    "try:\n",
    "    wandb.finish()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "pruning_cb = LitModelPruning(\n",
    "    pruning_fn=\"l1_unstructured\",\n",
    "    amount=0.1,\n",
    "    use_global_unstructured=True,\n",
    "    use_lottery_ticket_hypothesis=True,\n",
    "    apply_pruning=apply_prunning_every(5),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30,\n",
    "    logger=[wb_logger, tb_logger],\n",
    "    default_root_dir=\"logs\",\n",
    "    check_val_every_n_epoch=hparams[\"val_every\"],\n",
    "    callbacks=[L.pytorch.callbacks.EarlyStopping(monitor=\"valid/loss\", patience=3)],\n",
    "    accelerator=\"gpu\",\n",
    ")\n",
    "\n",
    "data = build_experiment(\n",
    "    corrupt_name=hparams['corrupt_name'],\n",
    "    corrupt_prob=hparams['corrupt_prob'],\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    ")\n",
    "train_loader = data[\"normal_labels\"][\"train_loader\"]\n",
    "val_loader = data[\"normal_labels\"][\"val_loader\"]\n",
    "\n",
    "# dict(model.named_parameters()).keys()\n",
    "\n",
    "hparams[\"learning_rate\"] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | net            | SmallAlexNet       | 56.8 M\n",
      "1 | train_acc      | MulticlassAccuracy | 0     \n",
      "2 | valid_acc      | MulticlassAccuracy | 0     \n",
      "3 | valid_top5_acc | MulticlassAccuracy | 0     \n",
      "4 | test_acc       | MulticlassAccuracy | 0     \n",
      "5 | test_top5_acc  | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "56.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.8 M    Total params\n",
      "227.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 196/196 [00:09<00:00, 19.64it/s, v_num=q4_6, valid/loss=1.600, valid/acc=0.733, valid/top5_acc=0.973, train/loss=0.0331, train/acc=0.989]\n",
      "Training took 42.17 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▃▃▆▆██</td></tr><tr><td>train/acc</td><td>▁▅▇█</td></tr><tr><td>train/loss</td><td>█▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▃▁▃▄▁▄▆▁▆█▁█</td></tr><tr><td>valid/acc</td><td>▁▅▇█</td></tr><tr><td>valid/loss</td><td>▁▃▅█</td></tr><tr><td>valid/top5_acc</td><td>▁▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train/acc</td><td>0.9895</td></tr><tr><td>train/loss</td><td>0.03314</td></tr><tr><td>trainer/global_step</td><td>783</td></tr><tr><td>valid/acc</td><td>0.73264</td></tr><tr><td>valid/loss</td><td>1.605</td></tr><tr><td>valid/top5_acc</td><td>0.97309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">alexnet_normal_labels_0.0</strong> at: <a href='https://wandb.ai/stepp1/pruning/runs/l0d5pfq4' target=\"_blank\">https://wandb.ai/stepp1/pruning/runs/l0d5pfq4</a><br/>Synced 7 W&B file(s), 4 media file(s), 11 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230628_111638-l0d5pfq4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb_logger = L.pytorch.loggers.TensorBoardLogger(hparams[\"log_dir\"], name=\"pruning\")\n",
    "wb_logger = L.pytorch.loggers.WandbLogger(project=\"pruning\", name=experiment_name)\n",
    "# wb_logger.watch(model)\n",
    "wandb.run.log_code(\".\")\n",
    "pl_model = LitModel(model, hparams=hparams)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.fit(\n",
    "    pl_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    ")\n",
    "print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
