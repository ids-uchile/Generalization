{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available corruptions:\n",
      " ['gaussian_pixels', 'random_labels', 'random_pixels', 'partial_labels', 'shuffled_pixels']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seed': 88,\n",
       " 'batch_size': 256,\n",
       " 'learning_rate': 0.1,\n",
       " 'epochs': 30,\n",
       " 'val_every': 1,\n",
       " 'log_dir': 'logs',\n",
       " 'dataset_name': 'cifar10',\n",
       " 'n_classes': 10,\n",
       " 'corrupt_name': 'normal_labels',\n",
       " 'corrupt_prob': 0,\n",
       " 'gradient_clipping': True,\n",
       " 'lr': 0.04,\n",
       " 'momentum': 0.9,\n",
       " 'weight_decay': 0.0,\n",
       " 'lr_scheduler': False}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generalization.utils.train import DEFAULT_PARAMS as hparams\n",
    "from generalization.randomization import available_corruptions\n",
    "\n",
    "print(\"Available corruptions:\\n\", available_corruptions())\n",
    "\n",
    "hparams['dataset_name'] = 'cifar10'\n",
    "hparams['n_classes'] = 10\n",
    "hparams['corrupt_name'] = 'normal_labels'\n",
    "hparams['corrupt_prob'] = 0\n",
    "hparams['gradient_clipping'] = True\n",
    "hparams['lr'] = 0.04\n",
    "hparams['momentum'] = 0.9\n",
    "hparams['weight_decay'] = 0.0\n",
    "hparams[\"lr_scheduler\"] = False\n",
    "\n",
    "hparams # same as generalization/configs/cifar-normal_labels.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data for Experiment: `normal_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_set': Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: /data/cifar10\n",
       "     Split: Train, Corruption: normal_labels,\n",
       " 'val_set': <torch.utils.data.dataset.Subset at 0x7efcd2697ac0>,\n",
       " 'test_set': <torch.utils.data.dataset.Subset at 0x7efcd2697b20>,\n",
       " 'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7efdecce3a00>,\n",
       " 'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7efcd2697a30>,\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x7efcd2697bb0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generalization.utils.data import build_experiment\n",
    "\n",
    "experiment_data = build_experiment(\n",
    "    0.0,\n",
    "    corrupt_name=\"normal_labels\",\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    ")\n",
    "experiment_data[\"normal_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Modules: `models` & `datamodule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alexnet', 'inception', 'mlp_1x512', 'mlp_3x512'])\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataModule:\n",
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: /data/cifar10\n",
       "    Split: Train, Corruption: normal_labels\n",
       "Val: <torch.utils.data.dataset.Subset object at 0x7efcc90be230>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generalization.models import get_cifar_models\n",
    "from generalization.utils.model import LitDataModule, LitModel\n",
    "\n",
    "models = get_cifar_models(lib=\"torch\")\n",
    "print(models.keys())\n",
    "\n",
    "\n",
    "dm = LitDataModule(hparams=hparams)\n",
    "dm.setup()\n",
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `Trainer` & `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstepp1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>logs/wandb/run-20230731_162442-alexnet-0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/stepp1/generalization-dense-normal_labels/runs/alexnet-0' target=\"_blank\">alexnet-0</a></strong> to <a href='https://wandb.ai/stepp1/generalization-dense-normal_labels' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stepp1/generalization-dense-normal_labels' target=\"_blank\">https://wandb.ai/stepp1/generalization-dense-normal_labels</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stepp1/generalization-dense-normal_labels/runs/alexnet-0' target=\"_blank\">https://wandb.ai/stepp1/generalization-dense-normal_labels/runs/alexnet-0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/step/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory logs/generalization-dense-normal_labels/alexnet-0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | net            | SmallAlexNet       | 56.8 M\n",
      "1 | train_acc      | MulticlassAccuracy | 0     \n",
      "2 | valid_acc      | MulticlassAccuracy | 0     \n",
      "3 | valid_top5_acc | MulticlassAccuracy | 0     \n",
      "4 | test_acc       | MulticlassAccuracy | 0     \n",
      "5 | test_top5_acc  | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "56.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.8 M    Total params\n",
      "227.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  20%|██        | 40/196 [00:02<00:07, 19.99it/s, v_num=et-0, valid/loss=1.060, valid/acc=0.482, valid/top5_acc=0.903, train/loss=1.020, train/acc=0.638] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  27%|██▋       | 52/196 [00:02<00:06, 20.83it/s, v_num=et-0, valid/loss=1.060, valid/acc=0.482, valid/top5_acc=0.903, train/loss=1.020, train/acc=0.638]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "def fit(trainer, model, datamodule):\n",
    "    torch.set_float32_matmul_precision(precision=\"medium\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.fit(model, datamodule)\n",
    "    print(f\"Training took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return trainer, model, datamodule\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # assure that logger process has exited\n",
    "    wandb.finish()\n",
    "    hparams[\"model_name\"] = model_name\n",
    "    log_dir = f\"logs/dense\"\n",
    "    project_name = f\"generalization-dense-{hparams['corrupt_name']}\"\n",
    "    experiment_name = f\"{hparams['model_name']}-{hparams['corrupt_prob']}\"\n",
    "\n",
    "    logger = WandbLogger(\n",
    "        name=experiment_name,\n",
    "        project=project_name,\n",
    "        log_model=\"all\",\n",
    "        save_dir=log_dir,\n",
    "        id=f\"{hparams['model_name']}-{hparams['corrupt_prob']}\",\n",
    "        group=f\"{hparams['corrupt_name']}\",\n",
    "        tags=[hparams[\"model_name\"], hparams[\"corrupt_name\"]],\n",
    "    )\n",
    "\n",
    "    ckpt = ModelCheckpoint(\n",
    "        # dirpath=ckpt_dir, # overridden by default_root_dir\n",
    "        filename=f\"{hparams['model_name']}-{hparams['corrupt_prob']}\"\n",
    "        + \"-{epoch:02d}-{valid/loss:.2f}\",\n",
    "        save_top_k=-1,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=hparams[\"epochs\"],\n",
    "        logger=logger,\n",
    "        callbacks=[ckpt],\n",
    "        default_root_dir=log_dir,\n",
    "        check_val_every_n_epoch=hparams[\"val_every\"],\n",
    "    )\n",
    "    pl_model = LitModel(\n",
    "        net=model,\n",
    "        hparams=hparams,\n",
    "    )\n",
    "\n",
    "    trainer, pl_model, dm = fit(trainer, pl_model, dm)\n",
    "\n",
    "    trainer.test(pl_model, dm.test_dataloader())\n",
    "\n",
    "    # trainer._save_to_state_dict(f\"logs/{project_name}/{experiment_name}/last.ckpt\")\n",
    "\n",
    "    # assure that logger process has exited\n",
    "    trainer.logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# from generalization.randomization.utils import CIFAR10_NORMALIZE_MEAN, CIFAR10_NORMALIZE_STD\n",
    "\n",
    "\n",
    "# idx = np.random.randint(len(experiments[CORRUPT_NAME][\"train_set\"]))\n",
    "\n",
    "# unnormalize = transforms.functional.normalize(\n",
    "#     experiments[CORRUPT_NAME][\"train_set\"][idx][0],\n",
    "#     mean=[-m / s for m, s in zip(CIFAR10_NORMALIZE_MEAN, CIFAR10_NORMALIZE_STD)],\n",
    "#     std=[1 / s for s in CIFAR10_NORMALIZE_STD],\n",
    "# )\n",
    "\n",
    "# label = experiments[CORRUPT_NAME][\"train_set\"][idx][1]\n",
    "# class_name = experiments[CORRUPT_NAME][\"train_set\"].classes[label]\n",
    "\n",
    "\n",
    "# f, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "# ax.imshow(unnormalize.permute(1, 2, 0))\n",
    "# ax.set_title(f\"Corrupted label: {class_name}\")\n",
    "# ax.axis(\"off\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
