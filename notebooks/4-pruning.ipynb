{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available corruptions:\n",
      " ['gaussian_pixels', 'random_labels', 'random_pixels', 'partial_labels', 'shuffled_pixels']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seed': 88,\n",
       " 'batch_size': 256,\n",
       " 'learning_rate': 0.1,\n",
       " 'epochs': 30,\n",
       " 'val_every': 1,\n",
       " 'log_dir': 'logs',\n",
       " 'dataset_name': 'cifar10',\n",
       " 'n_classes': 10,\n",
       " 'corrupt_name': 'normal_labels',\n",
       " 'corrupt_prob': 0,\n",
       " 'gradient_clipping': True,\n",
       " 'lr': 0.04,\n",
       " 'momentum': 0.9,\n",
       " 'weight_decay': 0.0,\n",
       " 'lr_scheduler': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generalization.utils.train import DEFAULT_PARAMS as hparams\n",
    "from generalization.randomization import available_corruptions\n",
    "\n",
    "print(\"Available corruptions:\\n\", available_corruptions())\n",
    "\n",
    "hparams['dataset_name'] = 'cifar10'\n",
    "hparams['n_classes'] = 10\n",
    "hparams['corrupt_name'] = 'normal_labels'\n",
    "hparams['corrupt_prob'] = 0\n",
    "hparams['gradient_clipping'] = True\n",
    "hparams['lr'] = 0.04\n",
    "hparams['momentum'] = 0.9\n",
    "hparams['weight_decay'] = 0.0\n",
    "hparams[\"lr_scheduler\"] = False\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "\n",
    "from generalization.utils import Classifier, LitDataModule\n",
    "from generalization.models import create_model\n",
    "\n",
    "dm = LitDataModule(hparams=hparams)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch_pruning as tp\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "\n",
    "class PruningClassifier(Classifier):\n",
    "    def __init__(self, net, hparams, pruner_params=None, pruner_entry=None):\n",
    "        super().__init__(net=net, hparams=hparams)\n",
    "        self.pruner_params = pruner_params\n",
    "        self.pruner_entry = pruner_entry\n",
    "\n",
    "        if self.hparams[\"prune\"]:\n",
    "            self.build_pruner()\n",
    "\n",
    "        self.pruner_stats = {\n",
    "            \"pruner/macs_ratio\": [],\n",
    "            \"pruner/macs\": [],\n",
    "            \"pruner/nparams\": [],\n",
    "            \"pruner/nparams_ratio\": [],\n",
    "            \"epoch\": [],\n",
    "            \"epoch_time\": [],\n",
    "        }\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
    "        if self.hparams[\"prune\"] and self.hparams[\"regularize\"]:\n",
    "            self.pruner.regularize(self.net)  # <== for sparse training\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, indices = batch\n",
    "        loss, logits, y = self.step(batch, batch_idx, reduction=\"mean\")\n",
    "\n",
    "        self.train_acc.update(logits, y)\n",
    "        acc = self.train_acc.compute().mean()\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        self.log(\"train/acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def net_device(self):\n",
    "        return next(self.net.parameters()).device\n",
    "\n",
    "    def build_pruner(self):\n",
    "        ignored_layers = []\n",
    "        n_out = self.hparams[\"n_classes\"]\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, torch.nn.Linear) and m.out_features == n_out:\n",
    "                ignored_layers.append(m)  # DO NOT prune the final classifier!\n",
    "\n",
    "            # DO NOT prune first convolutional layer\n",
    "            if isinstance(m, torch.nn.Conv2d) and m.in_channels == 3:\n",
    "                ignored_layers.append(m)\n",
    "\n",
    "        if self.pruner_entry is None:\n",
    "            self.pruner = (\n",
    "                tp.pruner.MetaPruner(  #  build using self.pruner_params and self.net\n",
    "                    model=self.net,\n",
    "                    ignored_layers=ignored_layers,\n",
    "                    **self.pruner_params,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.pruner = self.pruner_entry(\n",
    "                model=self.net,\n",
    "                ignored_layers=ignored_layers,\n",
    "                **self.pruner_params,\n",
    "            )\n",
    "            \n",
    "        example_inputs = self.pruner_params[\"example_inputs\"]\n",
    "        self.inputs_shape = [1] + list(example_inputs.shape[1:])\n",
    "        self.base_macs, self.base_nparams = tp.utils.count_ops_and_params(\n",
    "            self.net, example_inputs.to(self.net_device())\n",
    "        )\n",
    "\n",
    "        return self.pruner\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.previous_index = -1\n",
    "        if (\n",
    "            self.hparams[\"prune\"]\n",
    "            and not self.hparams[\"regularize\"]  # not sparse training\n",
    "            and self.current_epoch % self.hparams[\"prune_every_n_epoch\"] == 0\n",
    "        ):\n",
    "            # access trainloader\n",
    "            trainloader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "            if isinstance(self.pruner.importance, tp.importance.TaylorImportance):\n",
    "                batch = next(iter(trainloader))\n",
    "                inputs, targets, indices = batch\n",
    "                assert (\n",
    "                    self.previous_index != indices\n",
    "                ).all(), \"This is a hack, we need to use the same batch for pruning!\"\n",
    "                self.previous_index = indices\n",
    "                inputs, targets = inputs.to(self.net_device()), targets.to(\n",
    "                    self.net_device()\n",
    "                )\n",
    "                # print(\"inputs.shape\", inputs.shape)\n",
    "                # print(\"targets.shape\", targets.shape)\n",
    "                loss = self.loss(self(inputs), targets, reduction=\"mean\")\n",
    "                loss.backward()  # before pruner.step()\n",
    "\n",
    "            self.pruner.step()\n",
    "\n",
    "            x = torch.randn(self.inputs_shape)\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(\n",
    "                self.net, x.to(self.net_device())\n",
    "            )\n",
    "\n",
    "            self.pruner_stats[\"pruner/macs\"].append(macs)\n",
    "            self.pruner_stats[\"pruner/nparams\"].append(nparams)\n",
    "            self.pruner_stats[\"pruner/macs_ratio\"].append(macs / self.base_macs)\n",
    "            self.pruner_stats[\"pruner/nparams_ratio\"].append(\n",
    "                nparams / self.base_nparams\n",
    "            )\n",
    "            self.pruner_stats[\"epoch\"].append(self.current_epoch)\n",
    "\n",
    "            self.log(\"pruner/macs_ratio\", self.pruner_stats[\"pruner/macs_ratio\"][-1])\n",
    "            self.log(\n",
    "                \"pruner/nparams_ratio\", self.pruner_stats[\"pruner/nparams_ratio\"][-1]\n",
    "            )\n",
    "\n",
    "            # self.hparams[\"prune\"] = False\n",
    "            # decrease learning rate\n",
    "            self.trainer.optimizers[0].param_groups[0][\"lr\"] = (\n",
    "                self.trainer.optimizers[0].param_groups[0][\"lr\"] * 0.4\n",
    "            )\n",
    "\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        self.pruner_stats[\"epoch_time\"].append(time.time() - self.epoch_start_time)\n",
    "        self.log(\"epoch_time\", self.pruner_stats[\"epoch_time\"][-1], prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "cnn = create_model(\"inception\", lib=\"torch\", cifar=True)\n",
    "\n",
    "hparams['prune'] = False\n",
    "hparams['learning_rate'] = 0.04\n",
    "model = PruningClassifier(net=cnn, hparams=hparams, pruner_params=None)\n",
    "trainer = Trainer(max_epochs=7, devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 55.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10000000894069672    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.3034286499023438     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10000000894069672   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.3034286499023438    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "hparams[\"prune\"] = True\n",
    "hparams[\"learning_rate\"] = 0.003\n",
    "hparams[\"max_epochs\"] = 10\n",
    "hparams[\"prune_every_n_epoch\"] = 2\n",
    "hparams[\"sparsity_learning\"] = True\n",
    "hparams[\"regularize\"] = True\n",
    "\n",
    "# Importance criteria\n",
    "example_inputs = torch.randn(1, 3, 28, 28)\n",
    "imp = (\n",
    "    tp.importance.TaylorImportance()\n",
    ")  # or MagnitudeImportance, GroupNormPruner, BNScalePruner, etc.\n",
    "\n",
    "cnn = create_model(\"inception\", lib=\"torch\", cifar=True)\n",
    "\n",
    "imp = tp.importance.GroupNormImportance(\n",
    "    p=2, normalizer=\"max\"\n",
    ")  # normalized by the maximum score for CIFAR\n",
    "\n",
    "pruner_params = dict(\n",
    "    example_inputs=example_inputs,\n",
    "    global_pruning=True,\n",
    "    importance=imp,\n",
    "    ch_sparsity=1.0,\n",
    "    reg=5e-4,\n",
    "    iterative_steps=hparams[\"max_epochs\"] // hparams[\"prune_every_n_epoch\"],\n",
    ")\n",
    "\n",
    "pruner_entry = partial(tp.pruner.GroupNormPruner, **pruner_params)\n",
    "\n",
    "# load from checkpoint\n",
    "model = PruningClassifier(\n",
    "    # \"lightning_logs/version_2/checkpoints/epoch=6-step=1372.ckpt\",\n",
    "    net=cnn,\n",
    "    hparams=hparams,\n",
    "    pruner_params=pruner_params,\n",
    "    pruner_entry=pruner_entry,\n",
    ")\n",
    "trainer = Trainer(max_epochs=hparams[\"max_epochs\"], devices=1)\n",
    "trainer.test(model, datamodule=dm)\n",
    "model.build_pruner();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning: MACs = 152342618.0, n_params = 8047866\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | net       | InceptionSmall     | 8.0 M \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | valid_acc | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n",
      "32.191    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/196 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBefore pruning: MACs = \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mbase_macs\u001b[39m}\u001b[39;00m\u001b[39m, n_params = \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mbase_nparams\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdm)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:261\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    260\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    262\u001b[0m     trainer,\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    264\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    265\u001b[0m     batch_idx,\n\u001b[1;32m    266\u001b[0m     optimizer,\n\u001b[1;32m    267\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mPruningClassifier.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\u001b[39mself\u001b[39m, epoch, batch_idx, optimizer, optimizer_closure):\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mprune\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mregularize\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpruner\u001b[39m.\u001b[39;49mregularize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet)  \u001b[39m# <== for sparse training\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/generalization/lib/python3.10/site-packages/torch_pruning/pruner/algorithms/group_norm_pruner.py:151\u001b[0m, in \u001b[0;36mGroupNormPruner.regularize\u001b[0;34m(self, model, base)\u001b[0m\n\u001b[1;32m    149\u001b[0m     w \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[idxs]\n\u001b[1;32m    150\u001b[0m     g \u001b[39m=\u001b[39m w \u001b[39m*\u001b[39m scale\u001b[39m.\u001b[39mview( \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m([\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m(\u001b[39mlen\u001b[39m(w\u001b[39m.\u001b[39mshape)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) ) \u001b[39m#/ group_norm.view( -1, *([1]*(len(w.shape)-1)) ) * group_size #group_size #* scale.view( -1, *([1]*(len(w.shape)-1)) )\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     layer\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdata[idxs]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg \u001b[39m*\u001b[39m g \n\u001b[1;32m    152\u001b[0m     \u001b[39m#if layer.bias is not None:\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[39m#    b = layer.bias.data[idxs]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[39m#    g = b * scale\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39m#    layer.bias.grad.data[idxs]+=self.reg * g \u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39melif\u001b[39;00m prune_fn \u001b[39min\u001b[39;00m [\n\u001b[1;32m    157\u001b[0m     function\u001b[39m.\u001b[39mprune_conv_in_channels,\n\u001b[1;32m    158\u001b[0m     function\u001b[39m.\u001b[39mprune_linear_in_channels,\n\u001b[1;32m    159\u001b[0m ]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "print(f\"Before pruning: MACs = {model.base_macs}, n_params = {model.base_nparams}\")\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/step/mambaforge/envs/generalization/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/step/Code/projects/ids-generalization/notebooks/lightning_logs/version_18/checkpoints/epoch=9-step=1960.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/step/Code/projects/ids-generalization/notebooks/lightning_logs/version_18/checkpoints/epoch=9-step=1960.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 95.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7136176824569702     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.411431074142456     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7136176824569702    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.411431074142456    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 1.411431074142456, 'test/acc': 0.7136176824569702}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pruner_stats['epoch_time'] = model.pruner_stats['epoch_time'][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pruner/macs_ratio</th>\n",
       "      <th>pruner/macs</th>\n",
       "      <th>pruner/nparams</th>\n",
       "      <th>pruner/nparams_ratio</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.813656</td>\n",
       "      <td>123954495.0</td>\n",
       "      <td>6497719</td>\n",
       "      <td>0.807384</td>\n",
       "      <td>0</td>\n",
       "      <td>8.410430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.657220</td>\n",
       "      <td>100122591.0</td>\n",
       "      <td>5131958</td>\n",
       "      <td>0.637679</td>\n",
       "      <td>2</td>\n",
       "      <td>8.026077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.518770</td>\n",
       "      <td>79030777.0</td>\n",
       "      <td>3934190</td>\n",
       "      <td>0.488849</td>\n",
       "      <td>4</td>\n",
       "      <td>7.866646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396168</td>\n",
       "      <td>60353310.0</td>\n",
       "      <td>2889554</td>\n",
       "      <td>0.359046</td>\n",
       "      <td>6</td>\n",
       "      <td>7.700780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295554</td>\n",
       "      <td>45025522.0</td>\n",
       "      <td>2023650</td>\n",
       "      <td>0.251452</td>\n",
       "      <td>8</td>\n",
       "      <td>7.361772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pruner/macs_ratio  pruner/macs  pruner/nparams  pruner/nparams_ratio  \\\n",
       "0           0.813656  123954495.0         6497719              0.807384   \n",
       "1           0.657220  100122591.0         5131958              0.637679   \n",
       "2           0.518770   79030777.0         3934190              0.488849   \n",
       "3           0.396168   60353310.0         2889554              0.359046   \n",
       "4           0.295554   45025522.0         2023650              0.251452   \n",
       "\n",
       "   epoch  epoch_time  \n",
       "0      0    8.410430  \n",
       "1      2    8.026077  \n",
       "2      4    7.866646  \n",
       "3      6    7.700780  \n",
       "4      8    7.361772  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.pruner_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruner_params['iterative_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
